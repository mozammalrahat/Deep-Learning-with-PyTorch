{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dWkGcwZYyRUf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageFolder(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    super(ImageFolder, self).__init__()\n",
        "    self.data = []\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.class_names = os.listdir(root_dir)\n",
        "\n",
        "    for index, name in enumerate(self.class_names):\n",
        "      files = os.listdir(os.path.join(root_dir, name))\n",
        "      self.data +=list(zip(files, [index]*len(files)))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img_file, label = self.data[index]\n",
        "    root_and_dir = os.path.join(self.root_dir, self.class_names[label])  # Here label indicate which folder the first one or the second one \n",
        "    image = np.array(Image.open(os.path.join(root_and_dir,img_file)))\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image = image)\n",
        "      image = augmentations[\"image\"]\n",
        "\n",
        "\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "I0FBpTP9yyXL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([\n",
        "    A.Resize(width=1920, height=720),\n",
        "    A.RandomCrop(width=1280, height = 720),\n",
        "    A.Rotate(limit=45, p=0.9, border_mode = cv2.BORDER_CONSTANT),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1),\n",
        "    A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p = 0.9),\n",
        "    A.OneOf(\n",
        "        [\n",
        "         A.Blur(blur_limit=3, p=0.5),\n",
        "         A.ColorJitter(p=0.5),\n",
        "        ],\n",
        "        p=1.0,\n",
        "    ),\n",
        "    # this normalization is different from pytorch default normalization\n",
        "    A.Normalize(\n",
        "        mean = [0,0,0],\n",
        "        std = [1,1,1],\n",
        "        max_pixel_value=255,\n",
        "\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])"
      ],
      "metadata": {
        "id": "hvF0y2ZWy5d4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(root_dir=\"/content/images/cat_dogs\", transform = transform)\n",
        "for x, y in dataset:\n",
        "  print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0wArZts-h34",
        "outputId": "70e23156-f320-44e1-ebea-cddaab83f1d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n",
            "torch.Size([3, 720, 1280])\n"
          ]
        }
      ]
    }
  ]
}